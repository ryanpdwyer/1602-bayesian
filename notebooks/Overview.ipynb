{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interactive\n",
    "from bayeshelper import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference in Python\n",
    "Ryan Dwyer *February 17, 2016*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "![phase kick data](figs/phase-kick-151119.pdf)\n",
    "\n",
    "- Phase kick experimental data has complicated noise profile (see shaded regions)\n",
    "- Especially in organic samples with long recovery times, few data points, so worthwhile to glean as much information as possible from data\n",
    "\n",
    "\n",
    "*Bayesian inference allows inferring,*\n",
    "\n",
    "- **experimental noise**\n",
    "- **sample parameters**\n",
    "- **distribution of possible sample parameters and experimental noise**\n",
    "\n",
    "*simultaneously, in a motivated way.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics notation\n",
    "\n",
    "<img src=\"linear-fit.png\" width=640px>\n",
    "\n",
    "- Consider linear regression\n",
    "- $N$ data points $(x_i, y_i)$\n",
    "- Model the experimental data as a line with normally distributed errors,\n",
    "    $$\\begin{align}\n",
    "    \\mu_i& = m x_i + b&\\\\\n",
    "    y_i& \\sim \\mathcal{N}(\\mu_i, \\sigma)&\n",
    "    \\end{align}\n",
    "    $$\n",
    "    - $\\sim$ means \"is distributed as\"\n",
    "    - $\\mathcal{N}(\\mu, \\sigma)$ means a normal distribution with mean $\\mu$, standard deviation $\\sigma$\n",
    "    - This shows explicitly how to simulate data from the model.\n",
    "- Because $y_i$ is normally distributed, each data point has a likelihood $L_i$ given by the normal distribution's probability density (see above, right),\n",
    "    $$L_i = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( \\frac{-(y_i -\\mu_i)^2}{2\\sigma^2} \\right )$$\n",
    "- Independent data points, so the likelihood of the entire dataset is the product of the likelihood of individual data points,\n",
    "    $$\n",
    "    L = \\prod_{i=1}^{N} L_i\n",
    "    $$\n",
    "- Easier to work with the log likelihood,\n",
    "    $$\\log L = \\sum_{i=1}^{N} \\log L_i = -\\sum_{i=1}^{N} \\frac{(y_i -\\mu_i)^2}{2\\sigma^2} + \\log(\\sigma \\sqrt{2\\pi})$$\n",
    "- Ordinary least squares gives the *maximum likelihood estimate (MLE)* for this model (green)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian\n",
    "\n",
    "<img src=\"linear-fit.png\" width=640px>\n",
    "\n",
    "- **Prior** representing initial knowledge / belief about distribution of $m$, $b$, $\\sigma$\n",
    "- **Likelihood** derived from model (same as above)\n",
    "- **Posterior** Rather than a point estimate (MLE), compute a posterior distribution of plausible parameters values, given the **likelihood** and **prior**\n",
    "\n",
    "<img src=\"figs/likelihood.png\" alt=\"likelihood\" style=\"width:640px\">\n",
    "\n",
    "- $\\mathrm{Posterior} \\propto \\mathrm{Prior} \\times \\mathrm{Likelihood}$\n",
    "\n",
    "**Why?** Observed data may not provide much information about a given parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Brownian motion\n",
    "\n",
    "[[notebook](Brownian Motion Example.ipynb), [html](Brownian Motion Example.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python resources\n",
    "\n",
    "## `emcee`\n",
    "\n",
    "- Sampler needs no knowledge of derivatives\n",
    "\n",
    "## `PyStan`\n",
    "\n",
    "- Hamiltonian Monte Carlo (HMC) with \"No-U-turn sampler\" (NUTS) sampler\n",
    "\n",
    "## `PyMC2/3`\n",
    "\n",
    "- Metropolis-Hastings (PyMC2/PyMC3), HMC with NUTS (PyMC3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "\n",
    "- [Frequentism and Bayesianism: A Python-driven Primer.](http://arxiv.org/pdf/1411.5018v1.pdf)\n",
    "    - This is based on a series of blog posts at [Pythonic Permutations](https://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/). Most relevant is,\n",
    "- [Frequentism and Bayesianism IV: How to be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)\n",
    "\n",
    "### Books\n",
    "\n",
    "- [*Stan Modeling Language User's Guide and Reference Manual*](http://mc-stan.org/documentation/)\n",
    "    - Really great resource; implements basic versions of many different types of models in Stan (linear regression, time series analysis, multilevel modeling, ARMA processes, and more).\n",
    "- *Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan.* Kruschke, J.\n",
    "    - Starts from a fairly basic level. The author also has a variety of articles on his [website](http://www.indiana.edu/~kruschke/).\n",
    "- *Statistical Rethinking: A Bayesian Course with Examples in R and Stan.* McElreath, R [website](http://xcelab.net/rm/statistical-rethinking/).\n",
    "    - Emphasis on information theory for motivating choice of distributions, errors, priors.\n",
    "- Bayesian Data Analysis. Gelman, A., *et al*.\n",
    "    - Comprehensive overview, somewhat more advanced. Available online from the [Cornell library](https://newcatalog.library.cornell.edu/catalog/9204986)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [x] Motivation\n",
    "- [x] Statistics notation / terminology (with pictures)\n",
    "- [x] Brownian motion example [[notebook](LogNormalBrownian.ipynb), html]\n",
    "- [ ] Different samplers?\n",
    "- [ ] Scientific use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
